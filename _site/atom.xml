<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Debug Things</title>
 <link href="/atom.xml" rel="self"/>
 <link href="/"/>
 <updated>2014-09-23T22:44:14-07:00</updated>
 <id></id>
 <author>
   <name>James Davis</name>
   <email>james.l.davis@outlook.com</email>
 </author>

 
 <entry>
   <title>Bandwidth. It's not free...</title>
   <link href="//2014/09/23/bandwidth/"/>
   <updated>2014-09-23T00:00:00-07:00</updated>
   <id>/2014/09/23/bandwidth</id>
   <content type="html">&lt;p&gt;Usually when I performance test internal applications I don&amp;#39;t consider bandwidth unless I know we&amp;#39;re doing something irrational like transferring multi gigabyte files all the time. And even then I only consider it if we&amp;#39;re crossing a WAN or some other &amp;quot;slow&amp;quot; link between sites. But, every now and then there is a project that runs over the Internet and it is very data intensive. And, a lot of that data is out of the control of the developers (images, 3rd party libraries, fonts). &lt;/p&gt;

&lt;p&gt;It&amp;#39;s a given that it will cost you money, but how much? A dedicated link can run anywhere from $100 a month on a broadband backbone, all the way up to $50,000 for Gbit Level3 Fiber. How much money you want to spend on your connection depends on the amount of content you need to control.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;You may find that if you do some tuning on bandwidth you can save your self hundreds of thousands of dollars per year.&lt;/strong&gt; You read that right, &lt;strong&gt;hundreds of thousands of dollars&lt;/strong&gt;.&lt;/p&gt;

&lt;h3&gt;Background&lt;/h3&gt;

&lt;p&gt;Recently I was working on a launch of a new website and the main concern was hitting our requests per second target for daily use and what a holiday spike would look like. This is how things normally go. It&amp;#39;s easy. You drive load into the system until it eventually cracks. You suggest a fix. It gets implemented. Repeat.&lt;/p&gt;

&lt;p&gt;So, here we are about 3 weeks out from our go-live date. We spent months on tuning the system to run as lean as possible. We turned logging down. Compression on. SSL offloading. SQL tuning. Everything. We were ready.&lt;/p&gt;

&lt;p&gt;Yet, while doing all of this there wasn&amp;#39;t much visual content on the site. It wasn&amp;#39;t until this point in the project that &amp;quot;real&amp;quot; images and verbiage were coming in. When it did our bandwidth consumption jumped considerably. I don&amp;#39;t mean 2 times; 10 times. &lt;strong&gt;TEN TIMES&lt;/strong&gt;. We had to find out just how much data we were pushing, and we needed to know what it looked like from outside of the datacenter.&lt;/p&gt;

&lt;h3&gt;Bandwidth Estimation&lt;/h3&gt;

&lt;p&gt;Before we start running load tests willy nilly out on the open internet we need to do some math. Not hard math, but some fuzzy math to calculate our potential load. I will give some examples with a common website.&lt;/p&gt;

&lt;p&gt;For this I always use &lt;a href=&quot;http://www.telerik.com/fiddler&quot;&gt;Fiddler&lt;/a&gt;. There are some great plugins that will help you determine impact of change as well. Here is an &lt;a href=&quot;http://www.telerik.com/fiddler/web-app-performance-testing&quot;&gt;overview&lt;/a&gt; of what it can provide. These next few sections assume you&amp;#39;re somewhat familiar with Fiddler. I will include a few small steps but nothing in-depth. This isn&amp;#39;t a tutorial.&lt;/p&gt;

&lt;p&gt;When we first navigate to &lt;a href=&quot;http://www.microsoft.com/&quot;&gt;www.microsoft.com&lt;/a&gt; we will see the total amount of requests that are sent over. We can see this by selecting all of the requests that make up a page. In this case the home page. I have trimmed off some extra data for brevity.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Request Count:   51
Unique Hosts:    14
Bytes Sent:      36,471     (headers:36,471; body:0)
Bytes Received:  884,345        (headers:27,698; body:856,647)

ACTUAL PERFORMANCE
--------------
Requests started at:        16:34:22.077
Responses completed at:     16:34:25.057
Sequence (clock) duration:  00:00:02.9801705
Aggregate Session duration: 00:00:06.823
DNS Lookup time:            1,295ms
TCP/IP Connect duration:    1,181ms

.. response codes snipped ...

RESPONSE BYTES (by Content-Type)
--------------
               image/jpg: 382,186
application/x-javascript: 184,279
application/octet-stream: 86,289
               image/png: 70,863
         text/javascript: 66,832
               text/html: 40,260
               ~headers~: 27,698
                text/css: 19,181
               image/gif: 6,757

... hosts and timing estimates removed ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For my purposes a very important line was needed. In particular the amount of bytes. More over we need to convert this to bits.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Bytes Received:  884,345        (headers:27,698; body:856,647)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Armed with this information I could start calculating some of this fuzzy math. I knew that if we had 100Mbit of bandwidth and the majority of our users would get &lt;strong&gt;about&lt;/strong&gt; the same speed then we can use this formula going forward. This generates a theoretical maximum.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;((Bytes Received) * 8) / (Sequence (clock) duration) = effective bandwidth

(884,345B * 8b) / 2.98s = 2,374,080 = 2.4Mbit/sec

100Mbit/sec / 2.4Mbit/sec = 42 THEORETICAL simultaneous NEW page transfers in 3 seconds (14/sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In reality we more than likely won&amp;#39;t be able to hit 42. If we kind of squint our eyes and do some estimating we will need to reduce the number of actual page transfers. I always use the 80% rule. So really we could do about 34 new page loads at one time and be safe.&lt;/p&gt;

&lt;p&gt;Great. We have a number to shoot for. We&amp;#39;re done right? Well, no. If you went to your management and said, we can handle 34 pages per second, you might get a sideways look and be asked to explain.&lt;/p&gt;

&lt;p&gt;It may be more helpful to aggregate into minutes and even further translate that to users or sessions per minute. As you fuzz your numbers it may help make the overall point. Regardless of how fast the end user is there is a finite maximum number of pages that can be delivered in a finite amount of time.&lt;/p&gt;

&lt;p&gt;So, what do you do next? &lt;/p&gt;

&lt;h3&gt;Deep Estimation&lt;/h3&gt;

&lt;p&gt;Once you have done your naive estimation, you need to follow up and employ due diligence. Try and flow through the heaviest use cases and do the same math. If there aren&amp;#39;t any cases yet, make it up! Remember that what ever your QA team can&amp;#39;t find a user will find the first time.&lt;/p&gt;

&lt;p&gt;I believe that you have the same abilities. You, dear reader, can behave just like a user. Because, lets face it, you are a user deep in your soul. You order from Amazon, you poke around Ebay, you Google things wrong from time to time. Use this power. Embrace it. Go.&lt;/p&gt;

&lt;p&gt;Once you identify the pages, the transfer times, and estimated mixtures you can stop right? No. You need to go a step further and start estimating your return users and how that will affect caching. You are caching aren&amp;#39;t you?&lt;/p&gt;

&lt;h3&gt;Caching Estimation&lt;/h3&gt;

&lt;p&gt;Ah, yes. If you forgot, most modern browsers will adhere to caching rules. You need to use this to your advantage. Especially if you have some idea of how many users will be return.&lt;/p&gt;

&lt;p&gt;If we visit the same page(s) as before we can capture the same Fiddler statistics to get an idea of our cached content. If you look at the data below it looks as as if we have 20 times LESS data than before. So, in the naive approach we can transfer about 20 times the amount of page views when we have 100% return visitors. Here is an &lt;a href=&quot;http://msdn.microsoft.com/en-us/library/bb250442(v=vs.85).aspx&quot;&gt;excellent MSDN article&lt;/a&gt; on HTTP performance using Fiddler; read it.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Request Count:   17
Unique Hosts:    9
Bytes Sent:      15,501     (headers:15,501; body:0)
Bytes Received:  48,860     (headers:5,252; body:43,608)

ACTUAL PERFORMANCE
--------------
Requests started at:        17:03:17.465
Responses completed at:     17:03:19.177
Sequence (clock) duration:  00:00:01.7120980
Aggregate Session duration: 00:00:02.368
DNS Lookup time:            121ms
TCP/IP Connect duration:    876ms

RESPONSE CODES
--------------
HTTP/200:   14
HTTP/304:   3

RESPONSE BYTES (by Content-Type)
--------------
               text/html: 40,143
               ~headers~: 5,252
application/x-javascript: 2,978
         text/javascript: 401
               image/gif: 86

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the real world you won&amp;#39;t have that many return visitors. Depending on the type of requests you have (dyanmic v. static) and what your business mode is like (information or sales) you could be more in the camp of 30% - 50% return. You know your data best. I could offer some foolhardy estimates based on experience, but it would do you a disservice. Explore your data and know your numbers.&lt;/p&gt;

&lt;p&gt;Now, lets take this data and apply some fuzz to it and see what our effective rate at load would be.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;New users:
14 pages/sec * 60sec = 840pages/minute * 50% = 420

Return Users:
280 pages/sec * 60sec = 16800/minute * 50% = 8400

Total:
8820 pages per minute

Fuzzed number:
7056 pages per minute

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, this number sounds more like a enterprise level application. But, it seems kind of high.&lt;/p&gt;

&lt;h3&gt;User Estimation&lt;/h3&gt;

&lt;p&gt;Our goal is to find out if we have enough bandwidth to support our day one user base. Here is an example of our test case:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Actual Usage&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Normal Scenario

&lt;ul&gt;
&lt;li&gt; 5,000 Users &lt;/li&gt;
&lt;li&gt; 2 - 3 pages per session&lt;/li&gt;
&lt;li&gt; 3,000 sessions per minute&lt;/li&gt;
&lt;li&gt; 6,000 pages per minute&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt; Mobile

&lt;ul&gt;
&lt;li&gt; 500 Users&lt;/li&gt;
&lt;li&gt; 4 - 5 pages per session&lt;/li&gt;
&lt;li&gt; 1,000 session per minute&lt;/li&gt;
&lt;li&gt; 5,000 pages per minute&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Virtual Usage&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt; Normal Scenario

&lt;ul&gt;
&lt;li&gt; 1,000 virtual users&lt;/li&gt;
&lt;li&gt; 50% Return Visitors&lt;/li&gt;
&lt;li&gt; 3 - 5 pages per test&lt;/li&gt;
&lt;li&gt; 2,000 test per minute&lt;/li&gt;
&lt;li&gt; 6,000 pages per minute&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt; Mobile

&lt;ul&gt;
&lt;li&gt; 200 virtual users&lt;/li&gt;
&lt;li&gt; 40% Return Visitors&lt;/li&gt;
&lt;li&gt; 4 - 5 pages per test&lt;/li&gt;
&lt;li&gt; 1,000 tests per minute&lt;/li&gt;
&lt;li&gt; 4,000 pages per minute&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This test load simulated production traffic with less users than were reported on the site.  By sacrificing &amp;quot;actual&amp;quot; user load you can save some additional overhead of that many virtual users. This will come in handy later when you get to the bottom of the page.&lt;/p&gt;

&lt;h3&gt;Caution&lt;/h3&gt;

&lt;p&gt;We now have a page goal, and our possible load pattern. I urge you to exercise caution here. If you were keeping up and doing the math yourself you probably noticed were doing somewhere around 2600 requests per second. Even at 260 requests/sec per server your looking at 10 servers serving this web site. Is that too many, is it not enough? &lt;/p&gt;

&lt;p&gt;Don&amp;#39;t get caught up in the theoretical maximums. It is in everyone&amp;#39;s best interest to test as often as you can. Especially when you suggest a change, no matter how small. When in doubt, look at my previous blog post &lt;a href=&quot;/2014/09/16/always-performance-test/&quot;&gt;Always Performance Test!&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now is the time to apply some best practices before you test. Before you go all out on an external test, here are some low hanging fruit that will improve your bandwidth.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Caching

&lt;ul&gt;
&lt;li&gt;Make use of the caching tab in Fiddler&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Static Resource Sizes

&lt;ul&gt;
&lt;li&gt;Images&lt;/li&gt;
&lt;li&gt;CSS&lt;/li&gt;
&lt;li&gt;JS&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Page Sizes

&lt;ul&gt;
&lt;li&gt;Static&lt;/li&gt;
&lt;li&gt;Dynamic&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Bundling

&lt;ul&gt;
&lt;li&gt;Most web frameworks have a package that will bundle and minify&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Minification

&lt;ul&gt;
&lt;li&gt;If you can&amp;#39;t bundle, you should minify&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Compression

&lt;ul&gt;
&lt;li&gt;Above all else you should compress&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Content Delivery Network

&lt;ul&gt;
&lt;li&gt;If all else fails, seriously consider this&lt;/li&gt;
&lt;li&gt;Some common libraries are CDN&amp;#39;d for free, like jQuery on Google for instance&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Can We Test Now?!&lt;/h3&gt;

&lt;p&gt;Now is the time to start putting together a plan to test from the outside. I haven&amp;#39;t used any other services that integrate so well with a toolset I&amp;#39;m used to. VSO, provided a great platform to test on. I was even able to bring that data back into my local load test database.&lt;/p&gt;

&lt;p&gt;It&amp;#39;s no secret that when you use a Microsoft product it will cost you. But, in this case, you can easily map out the amount of money you need to spend.&lt;/p&gt;

&lt;p&gt;In the first part of the page I described my user and test mix. Essentially what I did was calculate how many pages per minute I wanted to achieve. Knowing my goal allowed me to save a lot of money. The going rate for testing in the cloud at the time of this writing is $0.002 per user minute. For example:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;1000 virtual users x 30 minutes = 30,000 virtual user minutes * $0.002 = $60
5000 virtual users x 30 minutes = 150,000 virtual user minutes * $0.002 = $300
5000 virtual users x 60 minutes = 300,000 virtual user minutes * $0.002 = $600
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, if you can afford some bad response times at the beginning of your test you should attempt to shorten your ramp-up time. So, if your normal user load takes 2 hours to be at capacity you can try shortening that to 10 or 20 minutes on an internal load test and see how your system handles it. Once you confirm you can take that kind of hit you should alter your load test to match that. Next, if your test had normally run for a couple of hours, you should consider only running for 30 minutes. &lt;/p&gt;

&lt;p&gt;I know, I know. That is not a great amount of testing, but it all depends on what your goals are.  If you have tons of disposable cash then by all means crank up 10,000 users and have them sit idle for hours on end.&lt;/p&gt;

&lt;h3&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;We all know bandwidth costs money. But you can &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Always Performance Test!</title>
   <link href="//2014/09/16/always-performance-test/"/>
   <updated>2014-09-16T00:00:00-07:00</updated>
   <id>/2014/09/16/always-performance-test</id>
   <content type="html">&lt;p&gt;While thumbing through Twitter I ran across an &lt;a href=&quot;http://apmblog.compuware.com/2014/09/16/detecting-bad-deployments-resource-impact-response-time-hotspot-garbage-collection/&quot;&gt;article&lt;/a&gt; written by &lt;a href=&quot;https://twitter.com/grabnerandi&quot;&gt;@grabnerandi&lt;/a&gt; about a company not being able to load test a change for dependency injection. I can&amp;#39;t tell you how many times in my job that performance testing has been brushed over for speed to market.&lt;/p&gt;

&lt;p&gt;Look, I get it. &lt;strong&gt;Money&lt;/strong&gt;. There, I said it. Most of the time the driving factor for not doing something you should be disciplined about is the bottom line. While I get it I don&amp;#39;t like it. I have had to inject myself into a release process to be the harbinger of doom. I try not to get all &amp;quot;the sky is falling,&amp;quot; but in reality it could.&lt;/p&gt;

&lt;p&gt;I know, I know. &lt;strong&gt;Time&lt;/strong&gt;. Right after money, but usually goes hand-in-hand, is time. This excuse gets even the best of us. Does anyone have time? No. Do we do it anyway? Sometimes. I&amp;#39;d like to say that while being the performance testing and DevOps advocate that I just ooze availability. But, I don&amp;#39;t. Just like everyone else I have constraints.&lt;/p&gt;

&lt;p&gt;What does it mean? Money + Time = &lt;strong&gt;Effort&lt;/strong&gt;. It always comes down to effort. This is the way to merge both time and money into one shortsighted mess. The effort required to test is usually deemed greater than the effort to just put it in. This is the death bringer of many of once stable applications.&lt;/p&gt;

&lt;h2&gt;Story Time&lt;/h2&gt;

&lt;p&gt;A while ago our team was asked to performance test a new web application. We got our grubby little hands on it we shredded the code and the database layout. Only to be met with &amp;quot;No one runs it like that.&amp;quot; Apparently we were &amp;quot;no one.&amp;quot; As a good performance testing team we provided solutions to the problems and the application team would make sure they went in. &lt;/p&gt;

&lt;p&gt;Ever since then, just about every dot release (x.1, x.2, etc.) has been tested by our team. A lot of things change from version to version, but we now had enough experience with the style of the developers and could keep up with the changes. Each time we found one or two things, but we rarely had major problems like we did in the first few releases. Our performance results always came back with positive results.&lt;/p&gt;

&lt;p&gt;Fast forward about 2 years. Multiple major and minor releases have come and went without issue. The application team had requested a new feature a few months ago and it was now ready for deployment. The change was &amp;quot;small&amp;quot; and carried only a couple of new screens. &lt;em&gt;Our team never even heard about it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;On go live, the applications started out fine but would always start performing poorly around 11am. This happened to be the peak time the application was used. A recycle would happen and the problem would go away for an hour and happen again. This process was repeated daily for a week. The application team was working with the vendor to find a solution.&lt;/p&gt;

&lt;p&gt;Our team got involved and we decided to take a look at what was going on. Since the application didn&amp;#39;t have any APM tools on it we went the traditional route and started monitoring performance counters. We went back in time and looked at the historical data. We could see that CPU was definitely becoming saturated. A bit deeper inspection into the .NET counters revealed that the &lt;a href=&quot;http://msdn.microsoft.com/en-us/library/x2tyfybc(v=vs.110).aspx&quot;&gt;# of Induced GC&lt;/a&gt; counter was going up at a steady rate.&lt;/p&gt;

&lt;p&gt;We compared the counters to a date just before the go-live and to no one&amp;#39;s surprise the issue was not there. We went back and performance tested the new application with existing scripts (we did not include the new screens) and the issue was still there. This showed there was something wrong in something else besides the proposed &amp;quot;only changes.&amp;quot;&lt;/p&gt;

&lt;p&gt;Luckily we had &lt;a href=&quot;http://www.compuware.com/en_us/application-performance-management.html&quot;&gt;dynaTrace&lt;/a&gt; at our disposal in our lower life cycle and we were able to quickly find the offending code. Analysis from the vendor showed a junior developer pulled from a previous branch that had a bug that was fixed long ago. A patch was issued that day by the vendor. We performance tested it. :)&lt;/p&gt;

&lt;h2&gt;Lesson&lt;/h2&gt;

&lt;h3&gt;Always Performance Test!&lt;/h3&gt;

&lt;p&gt;Considering all of the time, money, and effort (yes all three) wasted on: deploying bad code, end user impact, and time to resolution. It might have been quicker, easier, and cheaper to performance test this code.&lt;/p&gt;

&lt;p&gt;If you&amp;#39;re ever in a situation where one of these big three come up you should always make a statement, on record, about the need to performance test. If you are silent it&amp;#39;s not a great place to be if someone asks why it was never done. Being in DevOps you have just as much of a responsibility and burden to ensure application performance is as high as possible.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Writing code to write code</title>
   <link href="//2014/09/06/code-to-code/"/>
   <updated>2014-09-06T00:00:00-07:00</updated>
   <id>/2014/09/06/code-to-code</id>
   <content type="html">&lt;p&gt;Every now and then a balance needs to be struck from the every day coding of an application. While personal life is usually the topic for these types of posts, today that is not the case. Sometimes you have to write code in order to write code. This is somewhat of an extended rant from my previous post.&lt;/p&gt;

&lt;p&gt;While developing the core components of my &lt;a href=&quot;http://www.github.com/chainsapm&quot;&gt;ChainsAPM&lt;/a&gt; solution I felt the need to write down thoughts and musings of what I&amp;#39;m trying to do as well as explain some details of the implementation. Plus, I have other things to talk about aside from that project. At the same time I need to keep myself organized; so I needed to make a separate space.&lt;/p&gt;

&lt;p&gt;Behold, the humble blog. This has always been the best sounding board for getting ideas out there. While the normal blogging scene is great, there are a lot of gotchas and really some implementation details I don&amp;#39;t want to mess with. Places like Wordpress and blogger have excellent packages, software and templates. I have used these places before to bring to life some idea or useful tip. But, in all of this simplicity, lay a problem I never thought about. Once I signed up, I usually never signed back in once I wrote a couple of blogs.&lt;/p&gt;

&lt;p&gt;Part of the reason for this is beacause I never really used the software as it was intended. I didn&amp;#39;t connect with like minded bloggers, I didn&amp;#39;t share the pages and I was just overall uninterested in the process. I didn&amp;#39;t have much control over what the pages looked like unless I wanted to get into some Wordpress implementation details that I didn&amp;#39;t need.&lt;/p&gt;

&lt;p&gt;GitHub provides a very minimalistic blogging infrastructure based on Jekyll (which was written at GitHub). It&amp;#39;s a Ruby implemented web server that uses simple templating to make static content. Once you compile the Jekyll site you can publish it anywhere and not require any databases or other silly things just to deliver content.&lt;/p&gt;

&lt;p&gt;But, going this route meant I had to learn &lt;strong&gt;YET ANOTHER&lt;/strong&gt; thing in order to get some words on the internets. It&amp;#39;s starting to feel like Wordpress all over again. But, this time I have a vested interest because it&amp;#39;s somewhat of a unified front. I&amp;#39;m developing my blog site in my normal IDE using familiar tools and publishing to a place that I visit every day.&lt;/p&gt;

&lt;p&gt;So instead of just sluffing off the task I decided to learn this new-to-me skill because it was also more robust and allowed me to do things that would be difficult at best and impossible overall to implement. I am now able to combine efforts and cross post to my two blogs. &lt;/p&gt;

&lt;p&gt;After plowing through a month of coding in C++ I found myself wanting something a bit prettier, something that I could craft a bit more organically and not stress about all of the implementation details. In other words, I had to write this code in order to write other code.&lt;/p&gt;
</content>
 </entry>
 

</feed>
